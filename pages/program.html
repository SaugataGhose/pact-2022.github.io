{% extends "base-sidebar.html" %}

{% block maincol %}
  {% filter markdown %}
    # Program

    - [October 8: Tutorials and Workshops Day 1](#ws1)
    - [October 9: Tutorials and Workshops Day 2](#ws2)
    - [October 10: Main Conference Day 1](#main1)
    - [October 11: Main Conference Day 2](#main2)
    - [October 12: Main Conference Day 2](#main3)

    <a name="ws1">

    ## Saturday, October 8, 2022

    ##### Tutorial: Memory-Centric Computing

    - **Onur Mutlu** (ETH Zurich)

    Computing is bottlenecked by data. Large amounts of application data
    overwhelm storage capability, communication capability, and computation
    capability of the modern machines we design today. As a result, many key
    applications' performance, efficiency and scalability are bottlenecked by
    data movement. In this lecture, we describe three major shortcomings of
    modern architectures in terms of 1) dealing with data, 2) taking advantage
    of the vast amounts of data, and 3) exploiting different semantic
    properties of application data. We argue that an intelligent architecture
    should be designed to handle data well. We show that handling data well
    requires designing architectures based on three key principles: 1)
    data-centric, 2) data-driven, 3) data-aware. We give several examples for
    how to exploit each of these principles to design a much more efficient and
    high performance computing system. We especially discuss recent research
    that aims to fundamentally reduce memory latency and energy, and
    practically enable computation close to data, with at least two promising
    novel directions: 1) processing using memory, which exploits analog
    operational properties of memory chips to perform massively-parallel
    operations in memory, with low-cost changes, 2) processing near memory,
    which integrates sophisticated additional processing capability
    in memory controllers, the logic layer of 3D-stacked memory technologies,
    or memory chips to enable high memory bandwidth and low memory latency to
    near-memory logic. We show both types of architectures can enable orders of
    magnitude improvements in performance and energy consumption of many
    important workloads, such as graph analytics, database systems, machine
    learning, video processing. We discuss how to enable adoption of such
    fundamentally more intelligent architectures, which we believe are key to
    efficiency, performance, and sustainability. We conclude with some guiding
    principles for future computing architecture and system designs.

    ##### Tutorial: Boosting Productivity and Applications Performance on Parallel Distributed Systems with the SHAD C++ Library

    - **Vito Giovanni Castellana** (Pacific Northwest National Laboratory, Richland, WA)
    - **Marco Minutoli** (Pacific Northwest National Laboratory, Richland, WA)
    - **John Feo** (Pacific Northwest National Laboratory, Richland, WA)

    As the complexity and scale of High Performance Computing systems grows
    (node, core, and accelerators counts, memory, network), so does the
    complexity of applications, and thus, the demand for portability and
    productivity. With these issues in mind, we have designed SHAD, the
    Scalable High-performance Algorithms and Data-structures library. SHAD is
    open source software, written in C++, for C++ developers. Unlike other HPC
    libraries for distributed systems, which rely on SPMD models, SHAD adopts a
    shared-memory programming abstraction, to make C++ programmers feel at
    home. Thanks to its abstraction layers, SHAD can target different systems,
    ranging from laptops to HPC clusters, without any need for modifying the
    user-level code. In this tutorial, we first overview the design of the SHAD
    library, depicting its main components: runtime systems abstractions for
    tasking; parallel and distributed data-structures; STL-compliant interfaces
    and algorithms.  We then propose an interactive hands-on session, with
    coding exercises covering the different components of the software, from
    the tasking API up to the STL algorithms and Data Structures layer.
    The SHAD library is available at <https://github.com/pnnl/SHAD>.

    ##### Tutorial: NVMExplorer: A Framework for Cross-Stack Comparisons of Embedded Non-Volatile Memory Solutions

    - **Lilian Pentecost** (Amherst College)
    - **Alexander Hankin** (Harvard / Intel)
    - **Marco Donato** (Tufts)
    - **Mark Hempstead** (Tufts)
    - **Gu-Yeon Wei** (Harvard)
    - **David Brooks** (Harvard)

    NVMExplorer is a design space exploration framework that addresses key
    memory system design questions and reveals opportunities and optimizations
    for embedded NVMs under realistic system-level constraints, while providing
    a flexible interface and modular evaluation to empower further
    investigations.  This tutorial will walk through hands-on design studies
    using our open-source code base, give instruction for how to use our
    interactive data visualization dashboard, and highlight our most recent
    additions to the framework, including new data-intensive workload
    characteristics and 3D-integrated memory solutions.  We will additionally
    guide attendees to configure and run their own design studies according to
    their interests.  See our webpage (<http://nvmexplorer.seas.harvard.edu/>)
    for details.

    <a name="ws2">

    ## Sunday, October 9, 2022

    ##### Workshop: NextPIM --- Evolution of PIM for Next-Generation Computing

    - **Karthik Swaminathan** (IBM Research)
    - **Saransh Gupta** (IBM Research)
    - **Vijaykrishnan Narayanan** (Penn State University)

    Over the past decade or so, Processing in Memory (PIM) has evolved from
    single-cell demonstrations to large scale commercial deployment by leading
    processor and memory vendors. This workshop recognizes the pervasiveness of PIM
    across several application domains that form the backbone of computing systems
    today. It seeks to identify the successes, failures, and future opportunities
    for these technologies, potential new applications, and the development of
    tools, runtimes and other essential infrastructure that will dictate ongoing
    research this field.

    See the [workshop page](https://nextpim.github.io/2022/) for more information.

    ##### Tutorial: SODA Synthesizer: Accelerating Data Science Applications with an end-to-end Silicon Compiler

    - **Nicolas Bohm Agostini**
    - **Serena Curzel**
    - **Michele Fiorito**
    - **Vito Giovanni Castellana**
    - **Fabrizio Ferrandi**
    - **Antonino Tumeo**

    Data Science applications (machine learning, graph analytics) are among the
    main drivers for the renewed interests in designing domain specific
    accelerators, both for reconfigurable devices (Field Programmable Gate
    Arrays) and Application-Specific Integrated Circuits (ASICs). Today, the
    availability of new high-level synthesis (HLS) tools to generate
    accelerators starting from high-level specifications provides easier access
    to FPGAs or ASICs and preserves programmer productivity. However, the
    conventional HLS flow typically starts from languages such as C, C++, or
    OpenCL, heavily annotated with information to guide the hardware
    generation, still leaving a significant gap with respect to the (Python
    based) data science frameworks. 

    This tutorial will discuss HLS to accelerate data science on FPGAs or
    ASICs, highlighting key methodologies, trends, advantages, benefits, but
    also gaps that still need to be closed. The tutorial will provide a
    hands-on experience of the SOftware Defined Accelerators (SODA)
    Synthesizer, a toolchain composed of SODA-OPT, an opensource front-end and
    optimizer that interface with productive programming data science
    frameworks in Python, and Bambu, the most advanced open-source HLS tool
    available, able to generate optimized accelerators for data-intensive
    kernels.

    ##### Tutorial: SYCL for heterogenous computing: updates, experience, and feedback

    - **Zheming Jin** (ORNL)

    SYCL programming is based on standard ISO C++ with higher-level
    abstraction. It is a promising programming model for CPU, GPU, and other
    accelerators. The tutorial is organized as invited talks from researchers
    and developers in the SYCL community.  Whether or not you are familiar with
    SYCL, we hope that the tutorial will be interesting and valuable to your
    work. 

    <a name="main1">

    ## Monday, October 10, 2022

    ### Keynote: Closing the Gap between Quantum Algorithms and Machines with Hardware-Software Co-Design

    **Fred Chong** (Department of Computer Science, University of Chicago, Chicago, IL)

    Quantum computing is at an inflection point, where 127-qubit
    machines are deployed, and 1000-qubit machines are perhaps only a
    few years away.  These machines have the potential to fundamentally
    change our concept of what is computable and demonstrate practical
    applications in areas such as quantum chemistry, optimization,
    and quantum simulation.  Yet a significant resource gap remains
    between practical quantum algorithms and real machines.  A promising
    approach to closing this gap is to design software that is aware
    of the key physical properties of emerging quantum technologies.
    I will illustrate this approach with some of our recent work that
    focuses on techniques that break traditional abstractions and
    inform hardware design, including compiling programs directly
    to analog control pulses,  computing with ternary quantum bits,
    2.5D architectures for surface codes, and exploiting long-distance
    communication and tolerating atom loss in neutral-atom machines.

    > ![Fred Chong](images-generated/headshot-chong.jpg)
    >
    > Fred Chong is the Seymour Goodman Professor in the Department of
    > Computer Science at the University of Chicago and the Chief Scientist
    > for Quantum Software at ColdQuanta. He is also Lead Principal
    > Investigator for the EPiQC Project (Enabling Practical-scale Quantum
    > Computing), an NSF Expedition in Computing. Chong is a member of the
    > National Quantum Advisory Committee (NQIAC) which provides advice
    > to the President and Secretary of Energy on the National Quantum
    > Initiative Program. In 2020, he co-founded Super.tech, a quantum
    > software company, which was acquired by ColdQuanta in 2022. Chong
    > received his Ph.D. from MIT in 1996 and was a faculty member and
    > Chancellor's fellow at UC Davis from 1997-2005. He was also a
    > Professor of Computer Science, Director of Computer Engineering,
    > and Director of the Greenscale Center for Energy-Efficient Computing
    > at UCSB from 2005-2015. He is a recipient of the NSF CAREER award,
    > the Intel Outstanding Researcher Award, and 13 best paper awards.

    (Other talks to be announced)

    <a name="main2">

    ## Tuesday, October 11, 2022

    #### Keynote: MemComputing: Fundamentals and Applications

    **Massimiliano Di Ventra** (Department of Physics, University of California San Diego, La Jolla, CA)

    MemComputing is a new physics-based approach to computation
    that employs *time non-locality* (memory) to both process and
    store information on the same physical location. (M. Di Ventra,
    *MemComputing: Fundamentals and Applications*, Oxford University
    Press, 2022.) Its digital version is designed to solve combinatorial
    optimization problems. A practical realization of digital memcomputing
    machines (DMMs) can be accomplished via circuits of non-linear
    dynamical systems with memory engineered so that periodic orbits and
    chaos can be avoided. A given logic (or algebraic) problem is first
    mapped into this type of dynamical system whose point attractors
    represent the solutions of the original problem. A DMM then finds
    the solution via a succession of elementary avalanches (instantons)
    whose role is to eliminate configurations of logical inconsistency
    ("logical defects") from the circuit. I will discuss the physics
    behind MemComputing and show many examples of its applicability
    to various combinatorial optimization problems, Machine Learning,
    and Quantum Mechanics, demonstrating its advantages over traditional
    approaches and even quantum computing. Work supported by DARPA, DOE,
    NSF, CMRR, and [MemComputing, Inc](http://memcpu.com/).

    > ![Massimiliano Di Ventra](images-generated/headshot-diventra.jpg)
    >
    > Massimiliano Di Ventra obtained his undergraduate degree in Physics
    > *summa cum laude* from the University of Trieste (Italy) in 1991 and
    > did his PhD studies at the Swiss Federal Institute of Technology
    > in Lausanne in 1993-1997. He is now professor of Physics at the
    > University of California, San Diego. Di Ventra's research interests
    > are in condensed-matter theory and unconventional computing. He
    > has been invited to deliver more than 300 talks worldwide on these
    > topics. He has published more than 200 papers in refereed journals,
    > 4 textbooks, and has 7 granted patents (3 foreign). He is a fellow
    > of the IEEE, the American Physical Society, the Institute of Physics,
    > and a foreign member of Academia Europaea. In 2018 he was named Highly
    > Cited Researcher by Clarivate Analytics, he is the recipient of the
    > 2020 Feynman Prize for theory in Nanotechnology, and is a 2022 IEEE
    > Nanotechnology Council Distinguished Lecturer. He is the co-founder
    > of [MemComputing, Inc](http://memcpu.com/).

    (Other talks to be announced)

    <a name="main3">

    ## Wednesday, October 12, 2022

    #### Keynote: AI Acceleration:  Co-optimizing Algorithms, Hardware, and Software

    **Vijayalakshmi Srinivasan** (IBM Research, Yorktown Heights, NY)

    The combination of growth in compute capabilities and availability
    of large datasets has led to a re-birth of deep learning. Deep Neural
    Networks (DNNs) have become state-of-the-art in a variety of machine
    learning tasks spanning domains across vision, speech, and machine
    translation. Deep Learning (DL) achieves high accuracy in these
    tasks at the expense of 100s of ExaOps of computation. Hardware
    specialization and acceleration is a key enabler to improve
    operational efficiency of DNNs, in turn requiring synergistic
    cross-layer design across algorithms, hardware, and software.

    In this talk I will present this holistic approach adopted in the
    design of a multi-TOPs AI hardware accelerator. Key advances in
    the AI algorithm/application-level exploiting approximate computing
    techniques enable deriving low-precision DNNs models that maintain
    the same level of accuracy. Hardware performance-aware design space
    exploration is critical during compilation to map DNNs with diverse
    computational characteristics systematically and optimally while
    preserving familiar programming and user interfaces. The opportunities
    to co-optimize the algorithms, hardware, and the software provides
    the roadmap to continue to deliver superior performance over the
    next decade.

    > ![Vijayalakshmi Srinivasan](images-generated/headshot-srinivasan.jpg)
    >
    > Viji Srinivasan is a Distinguished Research Staff Member and a
    > manager of the accelerator architectures and compilers group at
    > the IBM T.J. Watson Research Center in Yorktown Heights. At IBM,
    > she has worked on various aspects of data management including
    > energy-efficient processor designs, microarchitecture of the memory
    > hierarchies of large-scale servers, cache coherence management
    > of symmetric multiprocessors, accelerators for data analytics
    > applications and more recently end-to-end accelerator solutions for
    > AI. Many of her research contributions have been incorporated into
    > IBM's Power and System-z Enterprise-class servers.

  {% endfilter %}
{% endblock %}
